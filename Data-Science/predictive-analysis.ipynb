{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b449b79",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: right\"> FIT1043 Introduction to Data Science </div> <div style=\"text-align: right\"> Assignment 2 </div>\n",
    "<div style=\"text-align: right\"> Name : Ooi Yu Zhang </div> <div style=\"text-align: right\"> Student ID : 32713339 </div> <div style=\"text-align: right\"> 25<sup>th</sup> April 2022 </div>\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67123fbe",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8effbc3",
   "metadata": {},
   "source": [
    "In this assignment, I will be predictive analysis on a dataset that consists of essays with a lot of varying attributes.</br>\n",
    "My approach will be to do research on different techniques and attempting to create the best model I am able to.</br>\n",
    "References will also be provided at the end of the assignment.\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64436c92",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3e617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, cohen_kappa_score\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4289ebf7",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f8ea4",
   "metadata": {},
   "source": [
    "## Reading The Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560b8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('FIT1043-Essay-Features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0a2eb",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062c26d",
   "metadata": {},
   "source": [
    "## Reading The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159e7b2",
   "metadata": {},
   "source": [
    "### Reading data from FIT1043 Essay Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e77077",
   "metadata": {},
   "source": [
    "#### Displaying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4d3e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1457</td>\n",
       "      <td>2153</td>\n",
       "      <td>426</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.053991</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>26.625000</td>\n",
       "      <td>423.995272</td>\n",
       "      <td>0.995294</td>\n",
       "      <td>207</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>105</td>\n",
       "      <td>0.246479</td>\n",
       "      <td>424</td>\n",
       "      <td>412</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>503</td>\n",
       "      <td>1480</td>\n",
       "      <td>292</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.068493</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>26.545455</td>\n",
       "      <td>290.993103</td>\n",
       "      <td>0.996552</td>\n",
       "      <td>148</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>77</td>\n",
       "      <td>0.263699</td>\n",
       "      <td>356</td>\n",
       "      <td>345</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253</td>\n",
       "      <td>3964</td>\n",
       "      <td>849</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>4.669022</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>17.326531</td>\n",
       "      <td>843.990544</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>285</td>\n",
       "      <td>0.335689</td>\n",
       "      <td>130</td>\n",
       "      <td>0.153121</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>988</td>\n",
       "      <td>210</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4.704762</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>207.653784</td>\n",
       "      <td>0.988828</td>\n",
       "      <td>112</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>62</td>\n",
       "      <td>0.295238</td>\n",
       "      <td>217</td>\n",
       "      <td>209</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1450</td>\n",
       "      <td>3139</td>\n",
       "      <td>600</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5.231667</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>594.652150</td>\n",
       "      <td>0.991087</td>\n",
       "      <td>255</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>165</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>702</td>\n",
       "      <td>677</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>1151</td>\n",
       "      <td>2404</td>\n",
       "      <td>467</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5.147752</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>21.227273</td>\n",
       "      <td>462.987069</td>\n",
       "      <td>0.991407</td>\n",
       "      <td>200</td>\n",
       "      <td>0.428266</td>\n",
       "      <td>113</td>\n",
       "      <td>0.241970</td>\n",
       "      <td>529</td>\n",
       "      <td>519</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>1015</td>\n",
       "      <td>1182</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4.904564</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>15.062500</td>\n",
       "      <td>238.655462</td>\n",
       "      <td>0.990272</td>\n",
       "      <td>94</td>\n",
       "      <td>0.390041</td>\n",
       "      <td>67</td>\n",
       "      <td>0.278008</td>\n",
       "      <td>293</td>\n",
       "      <td>283</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>1345</td>\n",
       "      <td>1814</td>\n",
       "      <td>363</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4.997245</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>27.923077</td>\n",
       "      <td>362.329640</td>\n",
       "      <td>0.998153</td>\n",
       "      <td>170</td>\n",
       "      <td>0.468320</td>\n",
       "      <td>107</td>\n",
       "      <td>0.294766</td>\n",
       "      <td>427</td>\n",
       "      <td>415</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>344</td>\n",
       "      <td>1427</td>\n",
       "      <td>287</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4.972125</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>22.076923</td>\n",
       "      <td>284.657277</td>\n",
       "      <td>0.991837</td>\n",
       "      <td>144</td>\n",
       "      <td>0.501742</td>\n",
       "      <td>83</td>\n",
       "      <td>0.289199</td>\n",
       "      <td>323</td>\n",
       "      <td>312</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>1077</td>\n",
       "      <td>2806</td>\n",
       "      <td>542</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.177122</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>24.636364</td>\n",
       "      <td>538.988889</td>\n",
       "      <td>0.994444</td>\n",
       "      <td>284</td>\n",
       "      <td>0.523985</td>\n",
       "      <td>155</td>\n",
       "      <td>0.285978</td>\n",
       "      <td>596</td>\n",
       "      <td>575</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1332 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essayid  chars  words  commas  apostrophes  punctuations  \\\n",
       "0        1457   2153    426      14            6             0   \n",
       "1         503   1480    292       9            7             0   \n",
       "2         253   3964    849      19           26             1   \n",
       "3         107    988    210       8            7             0   \n",
       "4        1450   3139    600      13            8             0   \n",
       "...       ...    ...    ...     ...          ...           ...   \n",
       "1327     1151   2404    467      16           10             0   \n",
       "1328     1015   1182    241       0           14             0   \n",
       "1329     1345   1814    363       5           11             0   \n",
       "1330      344   1427    287       5            8             0   \n",
       "1331     1077   2806    542      24            6             0   \n",
       "\n",
       "      avg_word_length  sentences  questions  avg_word_sentence         POS  \\\n",
       "0            5.053991         16          0          26.625000  423.995272   \n",
       "1            5.068493         11          0          26.545455  290.993103   \n",
       "2            4.669022         49          2          17.326531  843.990544   \n",
       "3            4.704762         12          0          17.500000  207.653784   \n",
       "4            5.231667         24          1          25.000000  594.652150   \n",
       "...               ...        ...        ...                ...         ...   \n",
       "1327         5.147752         22          0          21.227273  462.987069   \n",
       "1328         4.904564         16          0          15.062500  238.655462   \n",
       "1329         4.997245         13          3          27.923077  362.329640   \n",
       "1330         4.972125         13          1          22.076923  284.657277   \n",
       "1331         5.177122         22          3          24.636364  538.988889   \n",
       "\n",
       "      POS/total_words  prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0            0.995294           207                  0.485915            105   \n",
       "1            0.996552           148                  0.506849             77   \n",
       "2            0.994100           285                  0.335689            130   \n",
       "3            0.988828           112                  0.533333             62   \n",
       "4            0.991087           255                  0.425000            165   \n",
       "...               ...           ...                       ...            ...   \n",
       "1327         0.991407           200                  0.428266            113   \n",
       "1328         0.990272            94                  0.390041             67   \n",
       "1329         0.998153           170                  0.468320            107   \n",
       "1330         0.991837           144                  0.501742             83   \n",
       "1331         0.994444           284                  0.523985            155   \n",
       "\n",
       "      synonym_words/total_words  unstemmed  stemmed  score  \n",
       "0                      0.246479        424      412      4  \n",
       "1                      0.263699        356      345      4  \n",
       "2                      0.153121        750      750      4  \n",
       "3                      0.295238        217      209      3  \n",
       "4                      0.275000        702      677      4  \n",
       "...                         ...        ...      ...    ...  \n",
       "1327                   0.241970        529      519      4  \n",
       "1328                   0.278008        293      283      3  \n",
       "1329                   0.294766        427      415      3  \n",
       "1330                   0.289199        323      312      3  \n",
       "1331                   0.285978        596      575      4  \n",
       "\n",
       "[1332 rows x 19 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd3bf21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1457</td>\n",
       "      <td>2153</td>\n",
       "      <td>426</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.053991</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>26.625000</td>\n",
       "      <td>423.995272</td>\n",
       "      <td>0.995294</td>\n",
       "      <td>207</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>105</td>\n",
       "      <td>0.246479</td>\n",
       "      <td>424</td>\n",
       "      <td>412</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>503</td>\n",
       "      <td>1480</td>\n",
       "      <td>292</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.068493</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>26.545455</td>\n",
       "      <td>290.993103</td>\n",
       "      <td>0.996552</td>\n",
       "      <td>148</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>77</td>\n",
       "      <td>0.263699</td>\n",
       "      <td>356</td>\n",
       "      <td>345</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253</td>\n",
       "      <td>3964</td>\n",
       "      <td>849</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>4.669022</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>17.326531</td>\n",
       "      <td>843.990544</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>285</td>\n",
       "      <td>0.335689</td>\n",
       "      <td>130</td>\n",
       "      <td>0.153121</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>988</td>\n",
       "      <td>210</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4.704762</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>207.653784</td>\n",
       "      <td>0.988828</td>\n",
       "      <td>112</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>62</td>\n",
       "      <td>0.295238</td>\n",
       "      <td>217</td>\n",
       "      <td>209</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1450</td>\n",
       "      <td>3139</td>\n",
       "      <td>600</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5.231667</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>594.652150</td>\n",
       "      <td>0.991087</td>\n",
       "      <td>255</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>165</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>702</td>\n",
       "      <td>677</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essayid  chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0     1457   2153    426      14            6             0         5.053991   \n",
       "1      503   1480    292       9            7             0         5.068493   \n",
       "2      253   3964    849      19           26             1         4.669022   \n",
       "3      107    988    210       8            7             0         4.704762   \n",
       "4     1450   3139    600      13            8             0         5.231667   \n",
       "\n",
       "   sentences  questions  avg_word_sentence         POS  POS/total_words  \\\n",
       "0         16          0          26.625000  423.995272         0.995294   \n",
       "1         11          0          26.545455  290.993103         0.996552   \n",
       "2         49          2          17.326531  843.990544         0.994100   \n",
       "3         12          0          17.500000  207.653784         0.988828   \n",
       "4         24          1          25.000000  594.652150         0.991087   \n",
       "\n",
       "   prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0           207                  0.485915            105   \n",
       "1           148                  0.506849             77   \n",
       "2           285                  0.335689            130   \n",
       "3           112                  0.533333             62   \n",
       "4           255                  0.425000            165   \n",
       "\n",
       "   synonym_words/total_words  unstemmed  stemmed  score  \n",
       "0                   0.246479        424      412      4  \n",
       "1                   0.263699        356      345      4  \n",
       "2                   0.153121        750      750      4  \n",
       "3                   0.295238        217      209      3  \n",
       "4                   0.275000        702      677      4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a997eda",
   "metadata": {},
   "source": [
    "#### Checking the dimensions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19918096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1332, 19)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f9056",
   "metadata": {},
   "source": [
    "#### Checking how the column headers are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4281945b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['essayid', 'chars', 'words', 'commas', 'apostrophes', 'punctuations',\n",
       "       'avg_word_length', 'sentences', 'questions', 'avg_word_sentence', 'POS',\n",
       "       'POS/total_words', 'prompt_words', 'prompt_words/total_words',\n",
       "       'synonym_words', 'synonym_words/total_words', 'unstemmed', 'stemmed',\n",
       "       'score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be0804",
   "metadata": {},
   "source": [
    "#### Displaying more detailed parts of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb3269e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of       essayid  chars  words  commas  apostrophes  punctuations  \\\n",
       "0        1457   2153    426      14            6             0   \n",
       "1         503   1480    292       9            7             0   \n",
       "2         253   3964    849      19           26             1   \n",
       "3         107    988    210       8            7             0   \n",
       "4        1450   3139    600      13            8             0   \n",
       "...       ...    ...    ...     ...          ...           ...   \n",
       "1327     1151   2404    467      16           10             0   \n",
       "1328     1015   1182    241       0           14             0   \n",
       "1329     1345   1814    363       5           11             0   \n",
       "1330      344   1427    287       5            8             0   \n",
       "1331     1077   2806    542      24            6             0   \n",
       "\n",
       "      avg_word_length  sentences  questions  avg_word_sentence         POS  \\\n",
       "0            5.053991         16          0          26.625000  423.995272   \n",
       "1            5.068493         11          0          26.545455  290.993103   \n",
       "2            4.669022         49          2          17.326531  843.990544   \n",
       "3            4.704762         12          0          17.500000  207.653784   \n",
       "4            5.231667         24          1          25.000000  594.652150   \n",
       "...               ...        ...        ...                ...         ...   \n",
       "1327         5.147752         22          0          21.227273  462.987069   \n",
       "1328         4.904564         16          0          15.062500  238.655462   \n",
       "1329         4.997245         13          3          27.923077  362.329640   \n",
       "1330         4.972125         13          1          22.076923  284.657277   \n",
       "1331         5.177122         22          3          24.636364  538.988889   \n",
       "\n",
       "      POS/total_words  prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0            0.995294           207                  0.485915            105   \n",
       "1            0.996552           148                  0.506849             77   \n",
       "2            0.994100           285                  0.335689            130   \n",
       "3            0.988828           112                  0.533333             62   \n",
       "4            0.991087           255                  0.425000            165   \n",
       "...               ...           ...                       ...            ...   \n",
       "1327         0.991407           200                  0.428266            113   \n",
       "1328         0.990272            94                  0.390041             67   \n",
       "1329         0.998153           170                  0.468320            107   \n",
       "1330         0.991837           144                  0.501742             83   \n",
       "1331         0.994444           284                  0.523985            155   \n",
       "\n",
       "      synonym_words/total_words  unstemmed  stemmed  score  \n",
       "0                      0.246479        424      412      4  \n",
       "1                      0.263699        356      345      4  \n",
       "2                      0.153121        750      750      4  \n",
       "3                      0.295238        217      209      3  \n",
       "4                      0.275000        702      677      4  \n",
       "...                         ...        ...      ...    ...  \n",
       "1327                   0.241970        529      519      4  \n",
       "1328                   0.278008        293      283      3  \n",
       "1329                   0.294766        427      415      3  \n",
       "1330                   0.289199        323      312      3  \n",
       "1331                   0.285978        596      575      4  \n",
       "\n",
       "[1332 rows x 19 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43795ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>905.27027</td>\n",
       "      <td>2101.745495</td>\n",
       "      <td>424.485736</td>\n",
       "      <td>14.667417</td>\n",
       "      <td>8.141141</td>\n",
       "      <td>0.47973</td>\n",
       "      <td>4.939762</td>\n",
       "      <td>19.704204</td>\n",
       "      <td>1.222973</td>\n",
       "      <td>23.884687</td>\n",
       "      <td>420.596542</td>\n",
       "      <td>0.989935</td>\n",
       "      <td>198.913664</td>\n",
       "      <td>0.469164</td>\n",
       "      <td>110.16967</td>\n",
       "      <td>0.263846</td>\n",
       "      <td>468.987988</td>\n",
       "      <td>455.507508</td>\n",
       "      <td>3.427177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>526.68760</td>\n",
       "      <td>865.963750</td>\n",
       "      <td>171.873730</td>\n",
       "      <td>10.920781</td>\n",
       "      <td>6.124520</td>\n",
       "      <td>1.27168</td>\n",
       "      <td>0.231071</td>\n",
       "      <td>19.202731</td>\n",
       "      <td>1.847446</td>\n",
       "      <td>11.160020</td>\n",
       "      <td>170.985111</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>82.729266</td>\n",
       "      <td>0.052466</td>\n",
       "      <td>43.96192</td>\n",
       "      <td>0.038870</td>\n",
       "      <td>159.447449</td>\n",
       "      <td>155.751220</td>\n",
       "      <td>0.774275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.231322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.084112</td>\n",
       "      <td>35.647059</td>\n",
       "      <td>0.924771</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>0.027299</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>442.75000</td>\n",
       "      <td>1527.250000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.791679</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.142857</td>\n",
       "      <td>305.406284</td>\n",
       "      <td>0.987758</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.435709</td>\n",
       "      <td>81.00000</td>\n",
       "      <td>0.238423</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>350.750000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>914.50000</td>\n",
       "      <td>2029.500000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.946059</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.030331</td>\n",
       "      <td>406.982869</td>\n",
       "      <td>0.991572</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>0.465852</td>\n",
       "      <td>107.50000</td>\n",
       "      <td>0.262872</td>\n",
       "      <td>463.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1369.75000</td>\n",
       "      <td>2613.500000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.092938</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.048234</td>\n",
       "      <td>520.739458</td>\n",
       "      <td>0.994425</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>134.00000</td>\n",
       "      <td>0.288277</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>561.250000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1799.00000</td>\n",
       "      <td>6142.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>5.681429</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>1158.984563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>669.000000</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>355.00000</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          essayid        chars        words       commas  apostrophes  \\\n",
       "count  1332.00000  1332.000000  1332.000000  1332.000000  1332.000000   \n",
       "mean    905.27027  2101.745495   424.485736    14.667417     8.141141   \n",
       "std     526.68760   865.963750   171.873730    10.920781     6.124520   \n",
       "min       0.00000   169.000000    36.000000     0.000000     2.000000   \n",
       "25%     442.75000  1527.250000   310.000000     7.000000     4.000000   \n",
       "50%     914.50000  2029.500000   411.000000    13.000000     6.000000   \n",
       "75%    1369.75000  2613.500000   525.000000    21.000000    11.000000   \n",
       "max    1799.00000  6142.000000  1170.000000    72.000000    51.000000   \n",
       "\n",
       "       punctuations  avg_word_length    sentences    questions  \\\n",
       "count    1332.00000      1332.000000  1332.000000  1332.000000   \n",
       "mean        0.47973         4.939762    19.704204     1.222973   \n",
       "std         1.27168         0.231071    19.202731     1.847446   \n",
       "min         0.00000         2.231322     0.000000     0.000000   \n",
       "25%         0.00000         4.791679    13.000000     0.000000   \n",
       "50%         0.00000         4.946059    18.000000     1.000000   \n",
       "75%         0.00000         5.092938    24.000000     2.000000   \n",
       "max        26.00000         5.681429   642.000000    17.000000   \n",
       "\n",
       "       avg_word_sentence          POS  POS/total_words  prompt_words  \\\n",
       "count        1332.000000  1332.000000      1332.000000   1332.000000   \n",
       "mean           23.884687   420.596542         0.989935    198.913664   \n",
       "std            11.160020   170.985111         0.007308     82.729266   \n",
       "min             1.084112    35.647059         0.924771     14.000000   \n",
       "25%            19.142857   305.406284         0.987758    144.000000   \n",
       "50%            22.030331   406.982869         0.991572    193.000000   \n",
       "75%            26.048234   520.739458         0.994425    246.000000   \n",
       "max           303.000000  1158.984563         1.000000    669.000000   \n",
       "\n",
       "       prompt_words/total_words  synonym_words  synonym_words/total_words  \\\n",
       "count               1332.000000     1332.00000                1332.000000   \n",
       "mean                   0.469164      110.16967                   0.263846   \n",
       "std                    0.052466       43.96192                   0.038870   \n",
       "min                    0.288889       11.00000                   0.027299   \n",
       "25%                    0.435709       81.00000                   0.238423   \n",
       "50%                    0.465852      107.50000                   0.262872   \n",
       "75%                    0.500000      134.00000                   0.288277   \n",
       "max                    0.961207      355.00000                   0.465517   \n",
       "\n",
       "         unstemmed      stemmed        score  \n",
       "count  1332.000000  1332.000000  1332.000000  \n",
       "mean    468.987988   455.507508     3.427177  \n",
       "std     159.447449   155.751220     0.774275  \n",
       "min      48.000000    50.000000     1.000000  \n",
       "25%     361.000000   350.750000     3.000000  \n",
       "50%     463.000000   448.000000     3.000000  \n",
       "75%     581.000000   561.250000     4.000000  \n",
       "max     750.000000   750.000000     6.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d572199b",
   "metadata": {},
   "source": [
    "## Describing The Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714481b",
   "metadata": {},
   "source": [
    "\n",
    "### Measures of Central Tendency (Mean and Median)\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f046258d",
   "metadata": {},
   "source": [
    "#### Essay ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1107e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "905.2702702702703"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['essayid'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dbf58a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "914.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['essayid'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d96218",
   "metadata": {},
   "source": [
    "#### Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83b105c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2101.7454954954956"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['chars'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd7d1871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2029.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['chars'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6152fb",
   "metadata": {},
   "source": [
    "#### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "536687ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424.4857357357357"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['words'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "042b3a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['words'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81710c02",
   "metadata": {},
   "source": [
    "#### Commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28e7a013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.667417417417417"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['commas'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b2a5a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['commas'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ccf079",
   "metadata": {},
   "source": [
    "#### Apostrophes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "140d3a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.14114114114114"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['apostrophes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48da6466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['apostrophes'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a64c97",
   "metadata": {},
   "source": [
    "#### Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d207982b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4797297297297297"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['punctuations'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4684d534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['punctuations'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59379b4",
   "metadata": {},
   "source": [
    "#### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91f9b3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.939762160950457"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['avg_word_length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b4c5cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.946059456"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['avg_word_length'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c9f11",
   "metadata": {},
   "source": [
    "#### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b63b1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.704204204204203"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentences'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee2e2dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentences'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662f605",
   "metadata": {},
   "source": [
    "#### Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4be1367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.222972972972973"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['questions'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eab96517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['questions'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca2438e",
   "metadata": {},
   "source": [
    "#### Average Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5693185e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.884686874754497"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['avg_word_sentence'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "357eda8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.03033088"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['avg_word_sentence'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819558c",
   "metadata": {},
   "source": [
    "#### Part-Of-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9862a311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420.596541695675"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['POS'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0732115f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406.9828691"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['POS'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16bf6e8",
   "metadata": {},
   "source": [
    "#### Ratio of Part-Of-Speech to Total Number Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e82cd949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9899351385195204"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['POS/total_words'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9319890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9915724335"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['POS/total_words'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb14c6",
   "metadata": {},
   "source": [
    "#### Prompt Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb7dbdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198.91366366366367"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['prompt_words'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "614b1349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['prompt_words'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60588e71",
   "metadata": {},
   "source": [
    "#### Ratio of Prompt Words to Total Number Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2653b180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4691638682162157"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['prompt_words/total_words'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ee11984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.465851523"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['prompt_words/total_words'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880e40c",
   "metadata": {},
   "source": [
    "#### Synonym Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e2f67db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110.16966966966967"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['synonym_words'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "302446a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107.5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['synonym_words'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61137458",
   "metadata": {},
   "source": [
    "#### Ratio of Synonym Words to Total Number Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6177417a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2638456025600604"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['synonym_words/total_words'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd77938b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26287187050000005"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['synonym_words/total_words'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc774f",
   "metadata": {},
   "source": [
    "#### Unstemmed Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2614585a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468.987987987988"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['unstemmed'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "018c4ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['unstemmed'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ff9ae",
   "metadata": {},
   "source": [
    "#### Stemmed Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d93c330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455.5075075075075"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['stemmed'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "581749fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['stemmed'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8310e116",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0055afd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4271771771771773"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa5cae8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['score'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f22b4",
   "metadata": {},
   "source": [
    "### Measures of Variability (Range and Variance)\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc36272",
   "metadata": {},
   "source": [
    "#### Essay ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75ef8e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1799"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['essayid'].max() - dataset['essayid'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99b5c829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277399.8277255469"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['essayid'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a96e00",
   "metadata": {},
   "source": [
    "#### Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "691bd2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5973"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['chars'].max() - dataset['chars'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3bd458d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749893.2161705282"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['chars'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38c43c",
   "metadata": {},
   "source": [
    "#### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "727cc5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1134"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['words'].max() - dataset['words'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e46ba41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29540.57906008939"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['words'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44307b1d",
   "metadata": {},
   "source": [
    "#### Commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06690ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['commas'].max() - dataset['commas'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5238ae2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.26346049279931"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['commas'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323a0c4",
   "metadata": {},
   "source": [
    "#### Apostrophes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c189695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['apostrophes'].max() - dataset['apostrophes'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47bc04c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.50974114610453"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['apostrophes'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c5fff1",
   "metadata": {},
   "source": [
    "#### Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec92af7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['punctuations'].max() - dataset['punctuations'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a74468e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6171695737811695"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['punctuations'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39bff9",
   "metadata": {},
   "source": [
    "#### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "970c2f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4501067320000005"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['avg_word_length'].max() - dataset['avg_word_length'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f08f303a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05339401973194578"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['avg_word_length'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9265661",
   "metadata": {},
   "source": [
    "#### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "252c664b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "642"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentences'].max() - dataset['sentences'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "874a2007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368.74489591018533"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentences'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371ac9a",
   "metadata": {},
   "source": [
    "#### Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5afac1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['questions'].max() - dataset['questions'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b45211c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4130556176010507"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['questions'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc2d198",
   "metadata": {},
   "source": [
    "#### Average Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "552051ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301.91588785"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['avg_word_sentence'].max() - dataset['avg_word_sentence'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "208ae504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124.54604743033157"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['avg_word_sentence'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e5a1a",
   "metadata": {},
   "source": [
    "#### Part-Of-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20fdf7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123.33750418"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['POS'].max() - dataset['POS'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3c3400e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29235.908018788636"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['POS'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37587537",
   "metadata": {},
   "source": [
    "#### Ratio of Part-Of-Speech to Total Number Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f16539d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07522861199999997"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['POS/total_words'].max() - dataset['POS/total_words'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8dde551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.3412047135317936e-05"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['POS/total_words'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549124b",
   "metadata": {},
   "source": [
    "#### Prompt Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2b88cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['prompt_words'].max() - dataset['prompt_words'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c27c7cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6844.131533674911"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['prompt_words'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ac537",
   "metadata": {},
   "source": [
    "#### Ratio of Prompt Words to Total Number Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dd2cff21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.672318008"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['prompt_words/total_words'].max() - dataset['prompt_words/total_words'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "383084fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027526862975129464"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['prompt_words/total_words'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c0661",
   "metadata": {},
   "source": [
    "#### Synonym Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "560f5b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['synonym_words'].max() - dataset['synonym_words'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c10f5470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1932.6503791545117"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['synonym_words'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b629751",
   "metadata": {},
   "source": [
    "#### Ratio of Synonym Words to Total Number Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ad99ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43821839"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['synonym_words/total_words'].max() - dataset['synonym_words/total_words'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "336d3a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015109093517610816"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['synonym_words/total_words'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc341bab",
   "metadata": {},
   "source": [
    "#### Unstemmed Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8f297176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "702"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['unstemmed'].max() - dataset['unstemmed'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b162316d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25423.48896153852"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['unstemmed'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd9678",
   "metadata": {},
   "source": [
    "#### Stemmed Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e99d4cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['stemmed'].max() - dataset['stemmed'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e79ce61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24258.44246801268"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['stemmed'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729b9ef",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "46341415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['score'].max() - dataset['score'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "056c7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5995012668566343"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['score'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e861c",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a936d8",
   "metadata": {},
   "source": [
    "### What is Supervised Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709522ef",
   "metadata": {},
   "source": [
    "Supervised Learning which is also known as Supervised Machine Learning, is a subcategory of machine learning and artificial intelligence. Through the cross</br>-validation process, which is a technique where an untrained sample of the provided dataset is used to assess the machine learning model's performance, all the input data that is fed into the model will contribute towards \n",
    "the approximation of the desired output. **[1] [2]** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebef125",
   "metadata": {},
   "source": [
    "### What is the notion of labeled data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8344f01d",
   "metadata": {},
   "source": [
    "Labeled data is essentially data that have been labeled to be made identifiable according to specific properties, characteristics or classifications. In terms of supervised machine learning, labeled data acts as the starting point for training and testing machine learning models. It is used by models to perform analysis and comparisons betweem them to determine the nature of the data and sort them into their own individual categories. The result of this operation will be useful in the predictive analysis of unlabeled data wherein we will be able to predict the labels for these unlabeled data or in other words be able to categorize them. **[3]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e08c8c",
   "metadata": {},
   "source": [
    "### What are the purpose of training and test datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeac36a",
   "metadata": {},
   "source": [
    "The training and test datasets make up the one dataset in which they have been split into training and test datasets individually with different ratios for different purposes. It is very important that the training and test datasets are derived from the same dataset and that the test dataset is large enough to ensure that the results of the machine learning process are justifiable. In terms of the purpose of the datasets, the training dataset will be used to train the model while the test dataset will be used to test the trained model. Most importantly, the test dataset should never be used to train the model as this will result in a very high accuracy when the test dataset is tested using this kind of model, this scenario would be considered as overfitting. **[4]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359877c",
   "metadata": {},
   "source": [
    "### Splitting of Features and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d035d",
   "metadata": {},
   "source": [
    "#### Explanation for choice of features and labels:\n",
    "After looking at the dataset, it can be easily concluded that the labels would be the scores which we will be attempting to perform predicitive analysis on, and the features would be the rest of the data in the dataset. However, it can be seen that the the amount of data for the features is quite bloated, hence I will be trimming it down later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d19e0d",
   "metadata": {},
   "source": [
    "#### Obtaining the features (All data besides score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b988fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = dataset.iloc[:,:-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "99725f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.45700000e+03, 2.15300000e+03, 4.26000000e+02, ...,\n",
       "        2.46478873e-01, 4.24000000e+02, 4.12000000e+02],\n",
       "       [5.03000000e+02, 1.48000000e+03, 2.92000000e+02, ...,\n",
       "        2.63698630e-01, 3.56000000e+02, 3.45000000e+02],\n",
       "       [2.53000000e+02, 3.96400000e+03, 8.49000000e+02, ...,\n",
       "        1.53121319e-01, 7.50000000e+02, 7.50000000e+02],\n",
       "       ...,\n",
       "       [1.34500000e+03, 1.81400000e+03, 3.63000000e+02, ...,\n",
       "        2.94765840e-01, 4.27000000e+02, 4.15000000e+02],\n",
       "       [3.44000000e+02, 1.42700000e+03, 2.87000000e+02, ...,\n",
       "        2.89198606e-01, 3.23000000e+02, 3.12000000e+02],\n",
       "       [1.07700000e+03, 2.80600000e+03, 5.42000000e+02, ...,\n",
       "        2.85977860e-01, 5.96000000e+02, 5.75000000e+02]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74188523",
   "metadata": {},
   "source": [
    "#### Obtaining the labels (Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7302397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "af0fbdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 3, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fdea7",
   "metadata": {},
   "source": [
    "### Trimming down the Features (Using Feature Selection Methods)\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5fe295",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE)\n",
    "Recursive Feature Elimination essentially selects features by recursively considering smaller and smaller sets of features. It selects features based on the importance of the feature with regards to the predicting the output of the model. It eliminates features that are of less importance and selects the top features indicated. Along with RFE, I will also be using the Logistic Regression Algorithm which is a linear model for classification despite its name. In this model, a logistic function is used to model the probabilities describing the possible outcomes of a single trial, which in our case will be the features. **[5] [6] [7]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597a662",
   "metadata": {},
   "source": [
    "#### Explanation: Here I will be using Recursive Feature Elimination (RFE) to identify the top 9 features which is approximately half the features that contribute the most to predicting the Score. These 9 features will be used to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "905e34c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Contribution</th>\n",
       "      <th>Ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>essayid</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chars</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>words</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>commas</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apostrophes</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>punctuations</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>avg_word_length</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sentences</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>questions</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>avg_word_sentence</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>POS</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>POS/total_words</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>prompt_words</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>prompt_words/total_words</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>synonym_words</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>synonym_words/total_words</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>unstemmed</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature  Contribution  Ranking\n",
       "0                     essayid         False        4\n",
       "1                       chars          True        1\n",
       "2                       words          True        1\n",
       "3                      commas          True        1\n",
       "4                 apostrophes         False        5\n",
       "5                punctuations         False        8\n",
       "6             avg_word_length         False        3\n",
       "7                   sentences         False        2\n",
       "8                   questions         False        6\n",
       "9           avg_word_sentence          True        1\n",
       "10                        POS          True        1\n",
       "11            POS/total_words         False        7\n",
       "12               prompt_words          True        1\n",
       "13   prompt_words/total_words         False        9\n",
       "14              synonym_words          True        1\n",
       "15  synonym_words/total_words         False       10\n",
       "16                  unstemmed          True        1\n",
       "17                    stemmed          True        1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the 'lbfgs' solver is used here as it is the most robust solver\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, n_features_to_select=9)\n",
    "fit = rfe.fit(data_features, data_labels)\n",
    "d = {'Feature':dataset.columns[:-1], 'Contribution': fit.support_, 'Ranking': fit.ranking_}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1b9181",
   "metadata": {},
   "source": [
    "#### Result: The 9 features selected that will be used for training the model are 'chars', 'words', 'commas', avg_word_sentence', 'POS', 'prompt_words', 'synonym_words', 'unstemmed' and 'stemmed'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1fbd1",
   "metadata": {},
   "source": [
    "### Selecting The Features\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c2089184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Contribution</th>\n",
       "      <th>Ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chars</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>words</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>commas</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>avg_word_sentence</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>POS</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>prompt_words</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>synonym_words</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>unstemmed</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Feature  Contribution  Ranking\n",
       "1               chars          True        1\n",
       "2               words          True        1\n",
       "3              commas          True        1\n",
       "9   avg_word_sentence          True        1\n",
       "10                POS          True        1\n",
       "12       prompt_words          True        1\n",
       "14      synonym_words          True        1\n",
       "16          unstemmed          True        1\n",
       "17            stemmed          True        1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = df[df['Ranking'] == 1]\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "73b5b09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2153.,  426.,   14., ...,  105.,  424.,  412.],\n",
       "       [1480.,  292.,    9., ...,   77.,  356.,  345.],\n",
       "       [3964.,  849.,   19., ...,  130.,  750.,  750.],\n",
       "       ...,\n",
       "       [1814.,  363.,    5., ...,  107.,  427.,  415.],\n",
       "       [1427.,  287.,    5., ...,   83.,  323.,  312.],\n",
       "       [2806.,  542.,   24., ...,  155.,  596.,  575.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_feat = ['chars', 'words', 'commas','avg_word_sentence', 'POS', 'prompt_words', 'synonym_words', 'unstemmed', 'stemmed']\n",
    "new_features = dataset[sel_feat].values\n",
    "new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295019af",
   "metadata": {},
   "source": [
    "## Splitting the Training and Test Dataset\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37020c07",
   "metadata": {},
   "source": [
    "### Here the dataset will be split into training and test datasets using the sklearn.model_selection.train_test_split function. </br>\n",
    "### As a reminder for later on:</br> <span style=\"color:red\">*train_feat = Training dataset for Features*</span></br><span style=\"color:orange\">*test_feat = Test dataset for Features*</span></br><span style=\"color:blue\">*train_label = Training dataset for Labels*</span></br><span style=\"color:green\">*test_label = Test dataset for Labels*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0ca77483",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat, test_feat, train_label, test_label = train_test_split(new_features, data_labels, test_size=0.25, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1cac1723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.900e+03, 3.580e+02, 9.000e+00, ..., 8.000e+01, 4.460e+02,\n",
       "        4.320e+02],\n",
       "       [1.929e+03, 3.640e+02, 3.000e+00, ..., 1.160e+02, 3.400e+02,\n",
       "        3.240e+02],\n",
       "       [1.589e+03, 3.040e+02, 1.500e+01, ..., 8.300e+01, 2.840e+02,\n",
       "        2.700e+02],\n",
       "       ...,\n",
       "       [1.676e+03, 3.320e+02, 1.200e+01, ..., 9.700e+01, 3.990e+02,\n",
       "        3.880e+02],\n",
       "       [3.154e+03, 6.790e+02, 3.200e+01, ..., 2.000e+02, 6.960e+02,\n",
       "        6.780e+02],\n",
       "       [3.344e+03, 6.200e+02, 2.400e+01, ..., 1.380e+02, 6.180e+02,\n",
       "        6.000e+02]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d38390df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.841e+03, 3.700e+02, 2.100e+01, ..., 9.100e+01, 4.560e+02,\n",
       "        4.480e+02],\n",
       "       [1.491e+03, 3.280e+02, 0.000e+00, ..., 9.800e+01, 3.890e+02,\n",
       "        3.750e+02],\n",
       "       [2.404e+03, 4.670e+02, 1.600e+01, ..., 1.130e+02, 5.290e+02,\n",
       "        5.190e+02],\n",
       "       ...,\n",
       "       [1.965e+03, 4.220e+02, 2.300e+01, ..., 1.000e+02, 4.770e+02,\n",
       "        4.640e+02],\n",
       "       [2.228e+03, 4.550e+02, 2.100e+01, ..., 1.270e+02, 5.410e+02,\n",
       "        5.310e+02],\n",
       "       [1.174e+03, 2.320e+02, 2.000e+00, ..., 7.600e+01, 2.350e+02,\n",
       "        2.290e+02]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "256e4915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 3, 4, 3, 4, 4, 4, 3, 4, 4, 4, 2, 5, 3, 2, 4, 3, 3, 4, 2, 3,\n",
       "       4, 4, 3, 3, 4, 1, 3, 4, 3, 4, 4, 4, 3, 4, 4, 3, 5, 4, 3, 4, 4, 4,\n",
       "       3, 1, 3, 5, 3, 4, 3, 4, 3, 3, 3, 4, 3, 4, 3, 4, 4, 4, 2, 2, 2, 2,\n",
       "       3, 3, 3, 3, 1, 4, 2, 4, 3, 4, 1, 4, 4, 3, 4, 4, 3, 3, 3, 2, 4, 3,\n",
       "       3, 4, 3, 3, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 3,\n",
       "       3, 5, 3, 4, 3, 4, 1, 4, 4, 3, 3, 4, 4, 5, 4, 3, 3, 3, 3, 4, 3, 1,\n",
       "       4, 4, 1, 4, 4, 3, 3, 4, 3, 4, 4, 5, 5, 3, 3, 4, 4, 4, 3, 4, 4, 4,\n",
       "       3, 5, 3, 3, 3, 2, 4, 3, 3, 3, 4, 3, 4, 4, 4, 2, 3, 3, 4, 3, 4, 4,\n",
       "       3, 3, 3, 4, 3, 4, 4, 4, 3, 3, 4, 3, 4, 4, 3, 4, 3, 5, 2, 4, 4, 4,\n",
       "       4, 2, 3, 1, 5, 3, 4, 3, 4, 4, 5, 4, 3, 4, 4, 4, 3, 4, 3, 3, 4, 4,\n",
       "       3, 3, 3, 3, 4, 4, 3, 2, 4, 4, 5, 4, 2, 4, 3, 4, 2, 4, 3, 4, 3, 4,\n",
       "       3, 3, 4, 4, 2, 4, 4, 4, 3, 4, 3, 4, 2, 3, 5, 4, 3, 4, 3, 4, 4, 3,\n",
       "       4, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 2, 2, 4, 2, 4, 4, 3, 4, 3,\n",
       "       4, 2, 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, 3, 5, 3, 4, 3, 4, 3, 4, 5, 4,\n",
       "       4, 4, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 2, 4, 4, 3,\n",
       "       3, 2, 2, 4, 4, 3, 4, 3, 3, 2, 4, 2, 3, 3, 3, 4, 4, 1, 3, 3, 4, 4,\n",
       "       3, 4, 3, 5, 2, 3, 4, 5, 2, 3, 4, 3, 3, 3, 4, 3, 4, 4, 2, 4, 3, 3,\n",
       "       4, 4, 3, 3, 2, 2, 4, 3, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 3, 4, 3,\n",
       "       3, 2, 4, 4, 3, 3, 3, 3, 2, 3, 3, 3, 4, 4, 4, 4, 2, 4, 3, 3, 4, 3,\n",
       "       4, 1, 4, 2, 4, 2, 4, 3, 3, 3, 4, 3, 5, 3, 5, 4, 3, 4, 4, 1, 3, 3,\n",
       "       4, 4, 4, 4, 4, 3, 3, 3, 4, 3, 4, 3, 2, 4, 4, 4, 4, 2, 5, 3, 2, 4,\n",
       "       1, 4, 3, 3, 4, 2, 4, 2, 3, 4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 4, 4, 4,\n",
       "       3, 4, 4, 4, 4, 2, 2, 3, 4, 2, 3, 1, 3, 3, 4, 4, 4, 3, 4, 4, 4, 4,\n",
       "       4, 2, 3, 2, 3, 4, 4, 3, 3, 3, 3, 3, 5, 3, 3, 3, 4, 5, 4, 2, 2, 4,\n",
       "       3, 4, 3, 4, 4, 4, 5, 2, 4, 3, 4, 2, 3, 3, 4, 4, 4, 3, 4, 4, 4, 4,\n",
       "       4, 3, 4, 4, 4, 4, 5, 3, 4, 3, 3, 3, 5, 4, 3, 4, 3, 4, 4, 4, 3, 4,\n",
       "       3, 4, 4, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4, 4, 4, 2, 4, 3, 3, 3, 3, 3,\n",
       "       3, 2, 3, 4, 3, 4, 4, 3, 6, 3, 5, 3, 4, 3, 3, 5, 4, 3, 3, 4, 3, 4,\n",
       "       2, 4, 2, 4, 4, 3, 4, 3, 2, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 2, 4, 4,\n",
       "       3, 3, 4, 3, 5, 4, 4, 3, 4, 3, 2, 4, 3, 4, 4, 3, 2, 2, 5, 4, 2, 4,\n",
       "       3, 5, 3, 4, 3, 5, 3, 3, 3, 4, 2, 4, 5, 4, 3, 4, 3, 6, 4, 3, 4, 3,\n",
       "       3, 4, 4, 4, 5, 2, 3, 3, 3, 3, 4, 4, 3, 3, 2, 4, 3, 3, 4, 4, 3, 3,\n",
       "       3, 3, 3, 2, 4, 4, 4, 3, 4, 4, 4, 4, 3, 3, 2, 4, 5, 3, 3, 3, 4, 3,\n",
       "       3, 3, 4, 4, 2, 3, 3, 3, 4, 2, 4, 3, 3, 4, 2, 4, 5, 4, 4, 3, 3, 4,\n",
       "       4, 4, 3, 3, 4, 3, 5, 3, 2, 4, 4, 4, 3, 3, 4, 3, 3, 3, 4, 4, 4, 4,\n",
       "       4, 2, 3, 3, 4, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 3,\n",
       "       4, 4, 3, 4, 3, 4, 3, 3, 3, 1, 3, 4, 3, 3, 3, 4, 4, 3, 2, 3, 3, 4,\n",
       "       3, 3, 4, 4, 3, 4, 3, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 4, 5, 4, 4,\n",
       "       3, 4, 3, 4, 4, 4, 3, 4, 4, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4, 4, 3, 4,\n",
       "       2, 3, 4, 4, 3, 4, 4, 6, 5, 3, 4, 4, 4, 3, 4, 3, 3, 3, 3, 3, 4, 3,\n",
       "       3, 3, 3, 4, 5, 3, 4, 4, 2, 5, 4, 3, 3, 2, 4, 4, 4, 3, 3, 4, 5, 4,\n",
       "       3, 4, 4, 3, 3, 3, 2, 3, 2, 4, 4, 4, 3, 3, 4, 2, 5, 4, 3, 4, 4, 4,\n",
       "       4, 3, 4, 3, 2, 5, 3, 4, 4, 3, 2, 5, 3, 4, 4, 3, 4, 3, 4, 4, 2, 4,\n",
       "       3, 3, 4, 4, 3, 5, 3, 4, 3, 4, 4, 4, 5, 3, 2, 4, 4, 2, 3, 2, 3, 3,\n",
       "       3, 3, 3, 3, 3, 4, 3, 3, 4, 5, 4, 3, 4, 5, 4, 4, 4, 3, 4, 3, 3, 4,\n",
       "       4, 4, 3, 2, 4, 3, 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9e241d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 4, 4, 3, 4, 4, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 4, 4, 3, 3, 3,\n",
       "       4, 4, 3, 4, 3, 2, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 2, 5, 3,\n",
       "       3, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 5, 4, 4, 3, 2, 4, 3, 4, 2, 1, 3,\n",
       "       2, 4, 3, 3, 3, 3, 3, 4, 4, 2, 3, 3, 3, 4, 4, 3, 2, 3, 4, 3, 4, 5,\n",
       "       3, 2, 3, 4, 5, 3, 4, 4, 3, 4, 3, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 3,\n",
       "       3, 3, 3, 3, 4, 3, 4, 4, 3, 5, 3, 2, 4, 3, 5, 3, 3, 2, 2, 5, 4, 4,\n",
       "       4, 3, 4, 3, 3, 4, 3, 6, 4, 4, 4, 1, 4, 4, 3, 4, 3, 2, 4, 4, 3, 4,\n",
       "       4, 4, 4, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 4, 4, 3,\n",
       "       4, 3, 4, 3, 2, 4, 3, 4, 4, 4, 3, 4, 4, 3, 2, 4, 5, 2, 3, 4, 3, 1,\n",
       "       3, 3, 3, 4, 3, 4, 4, 4, 5, 4, 3, 4, 4, 4, 3, 5, 3, 3, 4, 3, 4, 3,\n",
       "       4, 4, 4, 3, 3, 3, 4, 4, 4, 2, 4, 3, 4, 2, 2, 4, 4, 4, 3, 2, 4, 3,\n",
       "       3, 3, 3, 4, 4, 4, 4, 2, 4, 3, 4, 4, 4, 3, 4, 4, 3, 3, 3, 4, 5, 3,\n",
       "       4, 4, 4, 4, 4, 4, 3, 2, 3, 3, 4, 3, 3, 3, 5, 3, 4, 3, 3, 4, 4, 3,\n",
       "       3, 4, 4, 3, 3, 2, 4, 3, 4, 3, 4, 3, 3, 3, 3, 3, 3, 4, 2, 2, 3, 4,\n",
       "       4, 4, 4, 3, 4, 3, 3, 3, 4, 3, 3, 4, 4, 1, 3, 4, 3, 3, 3, 3, 3, 3,\n",
       "       4, 4, 3], dtype=int64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452225f7",
   "metadata": {},
   "source": [
    "# Classification\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ce6a7",
   "metadata": {},
   "source": [
    "### What is the difference between binary and multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca8a5c0",
   "metadata": {},
   "source": [
    "Classification is the act of classifying data into different classes which is used to predict from which classes the input data are derived from. Binary classification and multi-class classification are two different types of classification.</br>\n",
    "\n",
    "<span style=\"color:orange\">__*Binary classification*__</span> is the classification of a dataset into two distinct classes. For example, you are presented a basket of fruits and vegetables which act as the dataset here and you will have to sort them into two batches, the two distinct classes here would obviously be Fruits and the other Vegetables. In fact, I have already performed binary classification earlier in this assignment which would be the application of Logistic Regression during Feature Selection. Other examples of algorithms used by binary classification are Decision Trees and Support Vector Machines, the latter of which we will be exploring later in the assignment as well.</br>\n",
    "\n",
    "<span style=\"color:blue\">__*Multi-class classification*__</span> is the classification of a dataset into many distinct classes, many being more than two. For example, instead of just a basket of fruits and vegetables from earlier, this time you are presented a basket of groceries to sort instead, there would be many distinct classes to sort them into, from Toiletries, to different types of Food classes, to Beverages and possibly to Meats. This is the main difference that sets multi-class classification from binary-classification wherein multi-class classification classifies a dataset into many distinct classes while binary classification just classifies a dataset into two distinct classes. Examples of algorithms used by multi-class classification are Decision Trees and Random Forests. **[8]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f152e9d",
   "metadata": {},
   "source": [
    "# Data Normalisation\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a4818",
   "metadata": {},
   "source": [
    "### What is the purpose of normalised / scaling of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e6abd",
   "metadata": {},
   "source": [
    "Data Normalisation is the process of scaling data to a standard scale. Due to the possibly broad range of values in given data, objective functions may not work properly in some machine learning algorithms without normalising data. Certain features may lose their effectiveness if their data is on a different scale when compared to other features. Hence, normalising data is important to ensure that all data is on a similar scale so that a good machine learning model can be produced. **[9]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd8c196",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e70e7",
   "metadata": {},
   "source": [
    "#### Use StandardScaler to normalise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b0c73e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "train_feat = sc.fit_transform(train_feat)\n",
    "test_feat = sc.transform(test_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d7c00",
   "metadata": {},
   "source": [
    "#### Checking if the data has been normalised appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "076988b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23062001, -0.38254297, -0.51644799, ..., -0.67255429,\n",
       "        -0.139495  , -0.14675677],\n",
       "       [-0.19750726, -0.34795727, -1.08164595, ...,  0.14280868,\n",
       "        -0.79744712, -0.83384211],\n",
       "       [-0.58572572, -0.69381424,  0.04874997, ..., -0.60460738,\n",
       "        -1.14504447, -1.17738478],\n",
       "       ...,\n",
       "       [-0.48638746, -0.53241432, -0.23384901, ..., -0.28752178,\n",
       "        -0.43122849, -0.42668043],\n",
       "       [ 1.20122102,  1.46779183,  1.6501442 , ...,  2.04532228,\n",
       "         1.41227888,  1.41827095],\n",
       "       [ 1.41816663,  1.12769914,  0.89654692, ...,  0.64108605,\n",
       "         0.92812543,  0.92204265]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "995388de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.29798733, -0.31337157,  0.61394794, ..., -0.42341561,\n",
       "        -0.07742404, -0.04496635],\n",
       "       [-0.69762398, -0.55547145, -1.36424493, ..., -0.26487281,\n",
       "        -0.49329944, -0.50938515],\n",
       "       [ 0.34485677,  0.24576386,  0.14294964, ...,  0.07486177,\n",
       "         0.37569393,  0.40672864],\n",
       "       ...,\n",
       "       [-0.15640177, -0.01362886,  0.80234726, ..., -0.21957486,\n",
       "         0.05292496,  0.05682407],\n",
       "       [ 0.14389663,  0.17659247,  0.61394794, ...,  0.39194737,\n",
       "         0.45017908,  0.48307146],\n",
       "       [-1.0595806 , -1.10884261, -1.17584561, ..., -0.76315018,\n",
       "        -1.44919215, -1.43822274]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005d39c",
   "metadata": {},
   "source": [
    "# SVM Classification Model\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4033eb",
   "metadata": {},
   "source": [
    "### What is SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2708fbf",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVMs) are supervised machine learning models that are used for classification and regression but mainly for classification. For classification, the main objective of the SVM algorithm is to find the most optimal hyperplane in an N-dimensional space that best classifies the input data where N is the number of features. To achieve this, the SVM algorithm finds the hyperplane with the maximum margin which is described as the maximum distance between input data of two classes. The hyperplanes mentioned are decision boundaries that help classify the input data, they are supported by support vectors which determine the position the hyperplanes are in. This very similar to a bridge supported by girders. **[10]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a8c23",
   "metadata": {},
   "source": [
    "### How does SVM/SVR compare to Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1db81",
   "metadata": {},
   "source": [
    "Linear Regression are also supervised machine learning models but are conversely used for regression only, hence their purposes are entirely different. Linear Regression is used to minimise the sum of squared errors while Support Vector Regression (SVR) minimises the coefficients instead of the squared error which is handled in the constraints where the absolute error is set to less than or equal to the maximum error. This gives SVR a flexibility advantage over Linear Regression as errors are disregarded within a certain limits. **[11]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0785d8",
   "metadata": {},
   "source": [
    "### What is the kernel in SVM/SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e60a65f",
   "metadata": {},
   "source": [
    "The kernel is a function used in SVM that transforms input data to the desired output data. It utilises pre-defined mathematical functions to perform complex calculations of datasets of any dimension to help the SVM algorithm decide on the hyperplane or in other terms the decision boundary. The kernel is also referred to as the kernel trick in which it will tend to solve a non-linearly separable problem with the help of linear classifying features. The kernel has values such as linear or polynomial that can be modified at will, this means that kernel can transmit the decision boundary to datasets of higher dimensions. The Radial Basis Function is the most commonly used kernel due to its robustness of being able to efficiently work with different sizes of datasets, however overfitting may occur on smaller datasets. Hence, linear or polynomial kernels should be used on smaller datasets instead. **[12]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a07a1c2",
   "metadata": {},
   "source": [
    "### Building The SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38f703",
   "metadata": {},
   "source": [
    "**Approach**: Here I will be using the sklearn.svm.SCV function to build the SVM model. The parameters for the function will be changed with different combinations and submitted to Kaggle for testing, the code here will be for the combination with the best results. The only parameters that will be tested are 'C' which is a regularization parameter and 'kernel' which is the type of kernel. My approach will be to take different numbers of different scales and changing values in small intervals once I obtain a value that is comparatively quite higher than others. The kernel will be interchanged between linear and and rbf as they are most suitable for the model that I will be building. **[13]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "876c5176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=8.5, kernel='linear', random_state=7)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(C = 8.5, kernel = 'linear', random_state = 7)\n",
    "svm.fit(train_feat, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aede349",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feaaf1b",
   "metadata": {},
   "source": [
    "### Predicting the Score using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "94f2a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = svm.predict(test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "31883ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 2, 4, 3, 4, 3, 3, 3,\n",
       "       4, 3, 4, 4, 3, 2, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 3, 4, 3,\n",
       "       4, 4, 3, 4, 3, 4, 3, 3, 4, 4, 3, 4, 4, 4, 3, 3, 4, 3, 4, 2, 3, 3,\n",
       "       3, 4, 4, 3, 2, 3, 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 4, 4, 4, 4,\n",
       "       3, 3, 3, 3, 4, 3, 3, 4, 3, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4,\n",
       "       4, 3, 3, 3, 4, 3, 4, 4, 3, 4, 3, 3, 4, 4, 4, 3, 4, 1, 3, 4, 3, 4,\n",
       "       4, 3, 3, 3, 3, 3, 3, 4, 4, 3, 4, 2, 4, 4, 3, 4, 3, 3, 4, 4, 3, 4,\n",
       "       4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3, 3, 3, 4, 3, 4, 4, 3, 3, 4, 3, 3,\n",
       "       4, 4, 4, 3, 3, 4, 3, 4, 4, 4, 3, 3, 4, 3, 2, 4, 4, 2, 4, 3, 4, 4,\n",
       "       3, 3, 3, 4, 4, 3, 4, 4, 4, 4, 3, 3, 3, 4, 3, 4, 3, 3, 3, 4, 3, 3,\n",
       "       4, 4, 4, 3, 3, 4, 4, 4, 3, 2, 4, 3, 4, 2, 3, 4, 4, 4, 2, 2, 4, 3,\n",
       "       3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 4, 4, 3, 4, 4, 3, 3, 4, 3, 4, 4, 3,\n",
       "       4, 4, 4, 4, 4, 4, 3, 2, 3, 3, 4, 3, 3, 4, 4, 3, 4, 4, 3, 4, 4, 3,\n",
       "       3, 4, 4, 3, 3, 2, 3, 3, 3, 3, 4, 3, 3, 3, 2, 3, 3, 4, 2, 3, 4, 4,\n",
       "       4, 4, 3, 3, 3, 4, 3, 2, 3, 3, 3, 3, 3, 1, 3, 3, 2, 3, 3, 3, 3, 4,\n",
       "       4, 4, 3], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9055e1",
   "metadata": {},
   "source": [
    "## Displaying the Confusion Matrix\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d635100b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2324c195970>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(test_label, prediction)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=[1,2,3,4,5,6])\n",
    "cmd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8a54b",
   "metadata": {},
   "source": [
    "**Result**: From the confusion matrix, it can be seen that most of the wrong predicted labels are just bordering the actual true labels, although not the best case scenario, it is still a decent outcome. However, there are 3 values that are outside of bordering the true labels which are the values in row 1 column 3 and 4, and the value in row 6 column 4. It can also been seen that most of the predicted values are for the scores 3 and 4, and fortunately for us, a lot of them seem to be correct or close predictions. However, you can also seen from the confusion matrix that the model did not predict any of the scores for 5 or 6 correctly which is quite the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537f07d",
   "metadata": {},
   "source": [
    "## Quadratic Weighted Kappa (QWK)\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d34619",
   "metadata": {},
   "source": [
    "### What is QWK?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c5c20",
   "metadata": {},
   "source": [
    "The Quadratic Weight Kappa (QWK) is a metric used to measure the agreement between two ratings. These ratings which can range from 0 to 5 are given according to how accurate the predicted data is to the actual data. QWK is calculated between the predicted scores and the actual scores. When calculating QWK, the main goal is to get as close to 1 as possible which is the maximum you can get, but there are also cases where the calculated QWK can fall below 0 and that is when there is less agreement between the ratings than expected. **[14]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fc1484",
   "metadata": {},
   "source": [
    "**Explanation**: Here I will be using the sklearn.metrics.cohen_kappa_score function with the parameter 'weights' set to 'Quadratic' to calculate the QWK which has been noted to be the same as QWK by the author in **[14]**. **[15]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "72c42afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6116089652625236"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(test_label, prediction, weights='quadratic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a1676f",
   "metadata": {},
   "source": [
    "**Result**: Here I received a QWK of 0.6116089652625236 which should be really good score as according to **[14]** a score of 0.6 or more is generally considered really good. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657fd3d3",
   "metadata": {},
   "source": [
    "# Kaggle Submission\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc801c7e",
   "metadata": {},
   "source": [
    "## Kaggle Submission Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d347e",
   "metadata": {},
   "source": [
    "### Reading the competition data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2e582107",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = pd.read_csv('FIT1043-Essay-Features-Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "386f8f32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1623</td>\n",
       "      <td>4332</td>\n",
       "      <td>900</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>4.813333</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>893.988852</td>\n",
       "      <td>0.993321</td>\n",
       "      <td>392</td>\n",
       "      <td>0.435556</td>\n",
       "      <td>196</td>\n",
       "      <td>0.217778</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143</td>\n",
       "      <td>1465</td>\n",
       "      <td>280</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.232143</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>278.321343</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>131</td>\n",
       "      <td>0.467857</td>\n",
       "      <td>51</td>\n",
       "      <td>0.182143</td>\n",
       "      <td>339</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>1696</td>\n",
       "      <td>325</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.218462</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>17.105263</td>\n",
       "      <td>321.316770</td>\n",
       "      <td>0.988667</td>\n",
       "      <td>178</td>\n",
       "      <td>0.547692</td>\n",
       "      <td>92</td>\n",
       "      <td>0.283077</td>\n",
       "      <td>352</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1596</td>\n",
       "      <td>2640</td>\n",
       "      <td>555</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>4.756757</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>19.821429</td>\n",
       "      <td>551.989150</td>\n",
       "      <td>0.994575</td>\n",
       "      <td>228</td>\n",
       "      <td>0.410811</td>\n",
       "      <td>107</td>\n",
       "      <td>0.192793</td>\n",
       "      <td>632</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>2844</td>\n",
       "      <td>596</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.771812</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>24.833333</td>\n",
       "      <td>593.658810</td>\n",
       "      <td>0.996072</td>\n",
       "      <td>279</td>\n",
       "      <td>0.468121</td>\n",
       "      <td>138</td>\n",
       "      <td>0.231544</td>\n",
       "      <td>626</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1226</td>\n",
       "      <td>1208</td>\n",
       "      <td>242</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4.991736</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>18.615385</td>\n",
       "      <td>237.327684</td>\n",
       "      <td>0.980693</td>\n",
       "      <td>135</td>\n",
       "      <td>0.557851</td>\n",
       "      <td>58</td>\n",
       "      <td>0.239669</td>\n",
       "      <td>244</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>862</td>\n",
       "      <td>4039</td>\n",
       "      <td>817</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4.943696</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>17.382979</td>\n",
       "      <td>812.656033</td>\n",
       "      <td>0.994683</td>\n",
       "      <td>386</td>\n",
       "      <td>0.472460</td>\n",
       "      <td>210</td>\n",
       "      <td>0.257038</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1562</td>\n",
       "      <td>2448</td>\n",
       "      <td>468</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.230769</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>21.272727</td>\n",
       "      <td>465.656652</td>\n",
       "      <td>0.994993</td>\n",
       "      <td>224</td>\n",
       "      <td>0.478632</td>\n",
       "      <td>101</td>\n",
       "      <td>0.215812</td>\n",
       "      <td>540</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1336</td>\n",
       "      <td>1081</td>\n",
       "      <td>214</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.051402</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>19.454545</td>\n",
       "      <td>212.990566</td>\n",
       "      <td>0.995283</td>\n",
       "      <td>114</td>\n",
       "      <td>0.532710</td>\n",
       "      <td>63</td>\n",
       "      <td>0.294393</td>\n",
       "      <td>259</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1171</td>\n",
       "      <td>2094</td>\n",
       "      <td>433</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4.836028</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>22.789474</td>\n",
       "      <td>426.651090</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>221</td>\n",
       "      <td>0.510393</td>\n",
       "      <td>121</td>\n",
       "      <td>0.279446</td>\n",
       "      <td>501</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essayid  chars  words  commas  apostrophes  punctuations  \\\n",
       "0       1623   4332    900      28           13             0   \n",
       "1       1143   1465    280      11            3             1   \n",
       "2        660   1696    325      17            2             0   \n",
       "3       1596   2640    555      20           17             0   \n",
       "4        846   2844    596      33            4             1   \n",
       "..       ...    ...    ...     ...          ...           ...   \n",
       "194     1226   1208    242       8            8             0   \n",
       "195      862   4039    817      24           11             1   \n",
       "196     1562   2448    468      22            7             0   \n",
       "197     1336   1081    214      14            5             0   \n",
       "198     1171   2094    433      11           12             0   \n",
       "\n",
       "     avg_word_length  sentences  questions  avg_word_sentence         POS  \\\n",
       "0           4.813333         39          1          23.076923  893.988852   \n",
       "1           5.232143         14          3          20.000000  278.321343   \n",
       "2           5.218462         19          1          17.105263  321.316770   \n",
       "3           4.756757         28          0          19.821429  551.989150   \n",
       "4           4.771812         24          9          24.833333  593.658810   \n",
       "..               ...        ...        ...                ...         ...   \n",
       "194         4.991736         13          0          18.615385  237.327684   \n",
       "195         4.943696         47          2          17.382979  812.656033   \n",
       "196         5.230769         22          0          21.272727  465.656652   \n",
       "197         5.051402         11          0          19.454545  212.990566   \n",
       "198         4.836028         19          0          22.789474  426.651090   \n",
       "\n",
       "     POS/total_words  prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0           0.993321           392                  0.435556            196   \n",
       "1           0.994005           131                  0.467857             51   \n",
       "2           0.988667           178                  0.547692             92   \n",
       "3           0.994575           228                  0.410811            107   \n",
       "4           0.996072           279                  0.468121            138   \n",
       "..               ...           ...                       ...            ...   \n",
       "194         0.980693           135                  0.557851             58   \n",
       "195         0.994683           386                  0.472460            210   \n",
       "196         0.994993           224                  0.478632            101   \n",
       "197         0.995283           114                  0.532710             63   \n",
       "198         0.985337           221                  0.510393            121   \n",
       "\n",
       "     synonym_words/total_words  unstemmed  stemmed  \n",
       "0                     0.217778        750      750  \n",
       "1                     0.182143        339      316  \n",
       "2                     0.283077        352      337  \n",
       "3                     0.192793        632      605  \n",
       "4                     0.231544        626      607  \n",
       "..                         ...        ...      ...  \n",
       "194                   0.239669        244      242  \n",
       "195                   0.257038        750      750  \n",
       "196                   0.215812        540      526  \n",
       "197                   0.294393        259      256  \n",
       "198                   0.279446        501      478  \n",
       "\n",
       "[199 rows x 18 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046a4f4",
   "metadata": {},
   "source": [
    "### Doing the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41938ebe",
   "metadata": {},
   "source": [
    "**Note**: sel_feat = ['chars', 'words', 'commas', 'avg_word_sentence', 'POS', 'prompt_words', 'synonym_words', 'unstemmed', 'stemmed'] from earlier in the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ce145ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_feat = sub_data[sel_feat].values\n",
    "kag_feat = sc.transform(sub_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aebab771",
   "metadata": {},
   "outputs": [],
   "source": [
    "kag_pred = svm.predict(kag_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d69a1e1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 3, 3, 3,\n",
       "       4, 4, 4, 4, 4, 4, 4, 3, 2, 4, 3, 3, 4, 3, 4, 4, 3, 3, 3, 3, 3, 3,\n",
       "       1, 3, 3, 4, 4, 3, 3, 4, 4, 4, 3, 4, 3, 3, 4, 4, 2, 3, 3, 4, 3, 3,\n",
       "       4, 4, 3, 4, 3, 3, 2, 3, 3, 3, 3, 4, 4, 4, 4, 3, 4, 3, 4, 2, 4, 4,\n",
       "       2, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 3, 3, 4, 2, 3, 3, 4, 3, 3, 4, 4,\n",
       "       4, 3, 4, 4, 4, 3, 3, 4, 2, 3, 4, 4, 4, 3, 2, 4, 3, 4, 3, 2, 4, 4,\n",
       "       2, 3, 4, 3, 3, 4, 1, 3, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 4, 3, 4, 3,\n",
       "       4, 3, 3, 4, 3, 4, 4, 3, 2, 3, 4, 4, 3, 3, 2, 4, 3, 4, 4, 3, 4, 3,\n",
       "       4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 4, 3, 4, 4, 3,\n",
       "       3], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kag_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85049c8a",
   "metadata": {},
   "source": [
    "### Reading the provided submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dd522e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1623</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1596</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1226</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>862</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1336</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1171</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essayid  score\n",
       "0       1623      4\n",
       "1       1143      3\n",
       "2        660      4\n",
       "3       1596      4\n",
       "4        846      4\n",
       "..       ...    ...\n",
       "194     1226      3\n",
       "195      862      4\n",
       "196     1562      4\n",
       "197     1336      3\n",
       "198     1171      3\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kag_sub = pd.read_csv('32713339-OoiYuZhang-29.csv')\n",
    "kag_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f8ab58ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1623</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1596</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1226</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>862</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1336</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1171</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essayid  score\n",
       "0       1623      4\n",
       "1       1143      3\n",
       "2        660      4\n",
       "3       1596      4\n",
       "4        846      4\n",
       "..       ...    ...\n",
       "194     1226      3\n",
       "195      862      4\n",
       "196     1562      4\n",
       "197     1336      3\n",
       "198     1171      3\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kag_sub['score'] = kag_pred\n",
    "kag_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79503d",
   "metadata": {},
   "source": [
    "### Output to the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a294500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kag_sub.to_csv(\"32713339-OoiYuZhang-29.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e22c80",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010849e2",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0d07b",
   "metadata": {},
   "source": [
    "Through this assignment, I have learnt a lot of new techniques and information, some I have applied in this assignment, and others that I know exist which may or may not be useful to me in the future. What I have learnt are the many different ways of feature selection, feature scaling, what a SVM and QWK are and how they work, and I had the opportunity to participate in my very first Kaggle competition which was quite interesting. Overall, I feel that the model I built could have been better, which is quite apparent when comparing my score to the rest of the leaderboard in the Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1694cf",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd1e068",
   "metadata": {},
   "source": [
    "**[1]**  IBM Cloud Education (2020, August 19). What is supervised learning? </br>Retrieved from https://www.ibm.com/cloud/learn/supervised-learning</br>\n",
    "**[2]**  Amal, Jo. (2021, July 21). What Is Cross-Validation? Comparing Machine Learning Models. </br>Retrieved from https://learn.g2.com/cross-validation</br>\n",
    "**[3]**  Techopedia (2020, July 1). What is Labeled Data?</br>Retrieved from https://www.techopedia.com/definition/33695/labeled-data</br>\n",
    "**[4]**  Google Developers (2020, February 10). Training and Test Sets: Splitting Data</br>Retrieved from https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data</br>\n",
    "**[5]**  Jason, B.(2020, August 28). Feature Selection For Machine Learning in Python</br>Retrieved from https://machinelearningmastery.com/feature-selection-machine-learning-python/M</br>\n",
    "**[6]**  Pedregosa et al.(2011a).Scikit-learn: Machine Learning in Python</br>Retrieved from https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE</br>\n",
    "**[7]**  Pedregosa et al.(2011b).Scikit-learn: Machine Learning in Python</br>Retrieved from https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression</br>\n",
    "**[8]**  Utsav, M.(2021, May 16). Binary and Multiclass Classification in Machine Learning</br>Retrieved from https://www.analyticssteps.com/blogs/binary-and-multiclass-classification-machine-learning</br>\n",
    "**[9]**  Pedregosa et al.(2011c).Scikit-learn: Machine Learning in Python</br>Retrieved from https://scikit-learn.org/stable/modules/preprocessing.html</br>\n",
    "**[10]**  Rohith, G.(2018, June 8). Support Vector Machine — Introduction to Machine Learning Algorithms</br>Retrieved from https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47</br>\n",
    "**[11]**  Tom, S.(2020, March 4). An Introduction to Support Vector Regression (SVR)</br>Retrieved from https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2</br>\n",
    "**[12]**  Python Geeks.(n.d.). SVM Kernel Function</br>Retrieved from https://pythongeeks.org/svm-kernel-function/</br>\n",
    "**[13]**  Pedregosa et al.(2011d).Scikit-learn: Machine Learning in Python</br>Retrieved from https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</br>\n",
    "**[14]**  Aman, A.(2018, December 30). Quadratic Kappa Metric explained in 5 simple steps</br>Retrieved from https://www.kaggle.com/code/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps/notebook</br>\n",
    "**[15]**  Pedregosa et al.(2011e).Scikit-learn: Machine Learning in Python</br>Retrieved from https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#r219a3b9132e1-1</br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
